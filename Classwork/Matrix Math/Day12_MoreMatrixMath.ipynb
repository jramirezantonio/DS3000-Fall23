{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DS3000 Day 12\n",
    "\n",
    "Oct 25/26, 2023\n",
    "\n",
    "Admin\n",
    "- Qwickly Attendance (PIN on board)\n",
    "- Homework 3 due **Oct 31**\n",
    "  - Homework 4 (Last Homework!) will be posted then\n",
    "- TAs Assigned to Project Teams (check spreadsheet)\n",
    "  - If you do not reach out to them by Friday night, they will reach out to you\n",
    "  - Data and Analysis Plan due **Nov 7** (example is posted)\n",
    "\n",
    "Push-Up Tracker\n",
    "- Section 04: 7\n",
    "- Section 05: 9\n",
    "- Section 06: 7\n",
    "\n",
    "Content (possibly over the next two-three class periods):\n",
    "- Matrices as transforms of vectors\n",
    "- Spans of vectors\n",
    "- Linear dependence and orthogonality\n",
    "- Projections\n",
    "  - Matrix inverses\n",
    " \n",
    "All builds up to line of best fit and linear regression (which includes some probability concepts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for today\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Math and Manipulations\n",
    "\n",
    "Assume you have two general matrices, $A$ which has shape $n \\times m$ and $B$ which has shape $p \\times q$. Some shapes are compatible for matrix multiplication, and many are not:\n",
    "\n",
    "- The **inner dimensions** must match to multiply matrices\n",
    "  - i.e. you may multiply a $2 \\times 3$ and a $3 \\times 4$ matrix. You may *not* multiply a $2 \\times 3$ and $2 \\times 3$ matrix.\n",
    "- **Order matters**\n",
    "  - based on the above, you should note that if $A$ is $2 \\times 3$ and $B$ is $3 \\times 4$, you may find $AB$ but **NOT** $BA$.\n",
    " \n",
    "**Also**: matrix multiplication is **NOT** pairwise multiplication. This should be obvious from the restrictions above. If you cannot multiply $2\\times3$ and $2\\times3$, then multiplication can't be that simple. So how do we do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3],\n",
    "             [-4, -5, -6]])\n",
    "B = np.array([[-5, 6, 1, -2],\n",
    "             [0, 1, 1, 0],\n",
    "             [8, 6, 4, -2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 19,  26,  15,  -8],\n",
       "       [-28, -65, -33,  20]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the numpy function that does matrix multiplication\n",
    "np.matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened? How does matrix multiplication work?\n",
    "\n",
    "Notice that the resulting matrix from multiplying $A$ by $B$ **kept the outer dimensions of the two matrices**. I.e. multiplying a $2\\times3$ matrix by a $3\\times4$ matrix resulted in a single $2\\times4$ matrix. This is because matrix multiplication is a result of:\n",
    "\n",
    "- Each element in the product matrix is the **dot product** of the corresponding **row from the left matrix** and **column from the right matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first row, first column\n",
    "np.dot(A[0,:], B[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, if vectors are matrices...\n",
    "\n",
    "As long as the inner dimensions match, we can multiply matrices. This means we can multiply vectors by matrices and matrices by vectors. In fact:\n",
    "\n",
    "- Matrix-vector multiplcation ($Ax$, for matrix $A$ and vector $x$) is a linear combination of the **rows** of $A$\n",
    "- Matrix-vector multiplcation ($xA$, for vector $x$ and matrix $A$) is a linear combination of the **columns** of $A$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$A = \\begin{bmatrix} 1 & 2 & 3 \\\\ -4 & -5 & -6 \\end{bmatrix}$$\n",
    "$$x = \\begin{bmatrix} 2 \\\\ 4 \\\\ -2 \\end{bmatrix}$$\n",
    "$$Ax = \\begin{bmatrix} 1 & 2 & 3 \\\\ -4 & -5 & -6 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 4 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 1(2) + 2(4) + 3(-2) \\\\ (-4)(2) + (-5)(4) + (-6)(-2) \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ -16 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4, -16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([2, 4, -2])\n",
    "np.matmul(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember that order matters; we cannot do xA because x is 3x1 and A is 2x3:\n",
    "#np.matmul(x, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-vector Multiplication as a Transformation\n",
    "\n",
    "Let us focus only on the situation where we have a matrix multiplied by a column vector, as above, but let's simplify (for now) to the situation where the matrix $A$ is a **square** matrix (i.e. same number of rows and columns). This means that the vector $x$ must have the same number of rows as $A$, and the resulting product $Ax$ will be of the same dimension as $x$:\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}$$\n",
    "$$x = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$$\n",
    "$$Ax = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 2(2) + 0(4) \\\\ 0(2) + 3(4) \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 12 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[2,0],\n",
    "              [0,3]])\n",
    "x = np.array([2, 4])\n",
    "np.matmul(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we think about this in terms of **how $A$ *changes* $x$**? Before applying the matrix $A$ to $x$, it was $x = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$, and afterwards it was $Ax = \\begin{bmatrix} 4 \\\\ 12 \\end{bmatrix}$, which we note is a vector of the same dimension. You can think of this graphically as $A$ **transforming** $x$.\n",
    "\n",
    "(Draw a picture on the board)\n",
    "\n",
    "Now what happens if we apply a different matrix, $B$? Perhaps something like:\n",
    "\n",
    "$$B = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$$\n",
    "$$x = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$$\n",
    "$$Bx = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0(2) + (-1)(4) \\\\ 1(2) + 0(4) \\end{bmatrix} = \\begin{bmatrix} -4 \\\\ 2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4,  2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([[0,-1],\n",
    "              [1,0]])\n",
    "np.matmul(B, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Draw a picture on the board)\n",
    "\n",
    "The values in the **diagonal** elements of $A$ seem to **scale** the vector $x$ when applied, and the values in the **off-diagonal** elements seem to **rotate** the vector $x$ when applied. So, if we combine them, what happens?\n",
    "\n",
    "$$A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}$$\n",
    "$$B = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$$\n",
    "$$x = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$$\n",
    "$$ABx = ??$$\n",
    "\n",
    "- Remember that order matters, so the above $ABx$ will **first rotate, then scale**, while $BAx$ will **first scale, then rotate**. Note that this *does* make a difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8,  6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(A, np.matmul(B, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-12,   4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(B, np.matmul(A, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Draw pictures on the board to illustrate)\n",
    "\n",
    "Finally note that if you tried to combine the two actions into one matrix, $C$, the matrix operations are not \"additive\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 14])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.array([[2, -1],\n",
    "              [1, 3]])\n",
    "np.matmul(C, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Spaces and Spans\n",
    "\n",
    "What is a vector space?\n",
    "\n",
    "- The coordinate planes defined by the dimensions make up the vector space; i.e. the number line makes up the 1-dimensional \"vector\" space, the $x-y$ axes make up the 2-dimensional vector space (a plane), while the $x-y-z$ axes make up the 3-dimensional vector space, etc.\n",
    "\n",
    "The **basis vectors** of a vector space are the vectors that \"define\" the direction of the axes, for example:\n",
    "\n",
    "- the $x-y$ plane has basis vectors: $\\hat{i} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and $\\hat{j} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$\n",
    "  - any other vector in 2-dimensional space can be reached by a linear combination of the two basis vectors.\n",
    " \n",
    "**Example:**\n",
    "\n",
    "$$\\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} = 3\\hat{i} + 4\\hat{j}$$\n",
    "$$\\begin{bmatrix} -23 \\\\ 42 \\end{bmatrix} = -23\\hat{i} + 42\\hat{j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ihat = np.array([1,0])\n",
    "jhat = np.array([0,1])\n",
    "3*ihat + 4*jhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-23,  42])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-23*ihat + 42*jhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use *other* vectors as \"basis\" vectors. For example, you can determine which coordinates can be reached by a linear combination of the following two vectors:\n",
    "\n",
    "$$v = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$$\n",
    "$$w = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$$\n",
    "\n",
    "This is called finding the **span** of a set of vectors. The **span of the basis vectors $\\hat{i}$ and $\\hat{j}$ is the entire $x-y$ plane**. How do you determine the span of a set of vectors? Use placeholders:\n",
    "\n",
    "$$\\alpha v + \\beta w = \\alpha \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix} + \\beta \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 2\\alpha + \\beta \\\\ 2\\beta \\end{bmatrix}$$\n",
    "\n",
    "This is a 2-dimensional vector where (using $x$ for the $x$-axis and $y$ for the $y$-axis), $x = 2\\alpha + \\beta$ or $y = 2\\beta$. These functions help us **define the span**; note that there are no restrictions on what $x$ and $y$ can be (given any choices of $\\alpha$ and $\\beta$), meaning that these two vectors' span is also the entire $x-y$ plane:\n",
    "\n",
    "$$y = 2(x - 2\\alpha) \\rightarrow y = 2x - 4\\alpha$$\n",
    "\n",
    "**Example (when the span is *not* the entire plane):**\n",
    "\n",
    "$$v = \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix}$$\n",
    "$$w = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$$\n",
    "$$\\alpha v + \\beta w = \\alpha \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix} + \\beta \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} -\\alpha + 2\\beta \\\\ -2\\alpha + 4\\beta \\end{bmatrix}$$\n",
    "\n",
    "Which means $x = -\\alpha + 2\\beta$ and $y = -2\\alpha + 4\\beta$, which in the 2-dimensional vector space is simply the **line $y=2x$**:\n",
    "\n",
    "$$y = -2\\alpha + 4\\beta = 2(-\\alpha + 2\\beta) = 2x$$\n",
    "\n",
    "**Fact/Note:** if a 2-d vector is a multiple of the other, you are guaranteed to have a line as the span of the two vectors (above, for example, $w = -2v$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spans In Summary\n",
    "\n",
    "In two-dimensions, there are three cases for the span of any set of 2-dimensional vectors:\n",
    "\n",
    "- Every point in the plane (see above)\n",
    "- A line passing through the origin (see above)\n",
    "- The origin (special case: the span of a set of origin vectors)\n",
    "\n",
    "In N-dimensions, there are *still* three cases for the span of any set of $N$-dimensional vectors:\n",
    "\n",
    "- Every point in the $N$-dimensional space\n",
    "- A reduced dimensionality space, passing through the origin (e.g. a plane or a line in 3-dimensions)\n",
    "- The origin\n",
    "\n",
    "**Finally**: the span of $N$ vectors is never more than $N$-dimensional space.\n",
    "\n",
    "- Example: The span of any single vector (of any dimension) is either a line or the origin (if it is the origin):\n",
    "\n",
    "$$v = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$$\n",
    "\n",
    "Even though $v$ exists in 3-dimensional space, the span of this single vector are any points reached by: $\\alpha v = \\begin{bmatrix} \\alpha \\\\ 2\\alpha \\\\ 3\\alpha \\end{bmatrix}$, which is the line $z = x + y$ in 3-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Dependence and Independence\n",
    "\n",
    "A set of vectors is **linearly dependent** if one of the vectors is a linear combination of the others:\n",
    "\n",
    "- i.e. if the span is a line (see above, and below)\n",
    "\n",
    "A set of vectors is **linearly independent** if each vector adds a new dimension to the span\n",
    "\n",
    "- see below for general idea\n",
    "\n",
    "**Linearly Dependent Vectors**:\n",
    "\n",
    "The set of vectors: $a = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, and $c = \\begin{bmatrix} 2 \\\\ 3 \\\\ 0 \\end{bmatrix}$ is linearly dependent because $c = 2a + 3b$.\n",
    "\n",
    "**Linearly Independent Vectors**:\n",
    "\n",
    "The set of vectors: $a = \\begin{bmatrix} \\alpha \\\\ 0 \\end{bmatrix}$ and $b = \\begin{bmatrix} \\beta \\\\ \\text{Anything Non-Zero} \\end{bmatrix}$ for any $\\alpha$ and $\\beta$ is linearly independent.\n",
    "\n",
    "**Fact/Note:** $N+1$ or more vectors of length $N$ are linearly dependent. Example:\n",
    "\n",
    "The set of vectors: $a = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$, $b = \\begin{bmatrix} -4 \\\\ 6 \\end{bmatrix}$, and $c = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ is linearly dependent because $b = -2a + 8c$.\n",
    "\n",
    "- This is actually a very important point for machine learning with data. When you have more **features** than **observations** (this is called the [Large p, small n problem](https://www.google.com/search?q=Large+p%2C+small+n+problem&sca_esv=576533920&sxsrf=AM9HkKl_8u1hxRNyO9OTf4YbeWyD68GxSw%3A1698255238798&ei=hlE5ZduOMIKU5OMPj5mxOA&ved=0ahUKEwjb6fvh3ZGCAxUCCnkGHY9MDAcQ4dUDCBA&uact=5&oq=Large+p%2C+small+n+problem&gs_lp=Egxnd3Mtd2l6LXNlcnAiGExhcmdlIHAsIHNtYWxsIG4gcHJvYmxlbTIFEAAYgAQyCBAAGIoFGIYDMggQABiKBRiGAzIIEAAYigUYhgNIqSdQ0AZYuiZwBHgBkAEBmAHsAaABpRWqAQYxNy42LjK4AQPIAQD4AQHCAgoQABhHGNYEGLADwgIEECMYJ8ICBxAuGIoFGCfCAggQABiKBRiRAsICCxAuGIoFGLEDGIMBwgIREC4YgAQYsQMYgwEYxwEY0QPCAgcQIxiKBRgnwgILEC4YgwEYsQMYgATCAgsQABiKBRixAxiDAcICCxAAGIAEGLEDGIMBwgIOEC4YgAQYxwEYrwEYjgXCAggQLhiABBixA8ICDhAuGIAEGLEDGMcBGNEDwgIIEAAYgAQYsQPCAgoQABiABBgUGIcCwgIGEAAYFhge4gMEGAAgQYgGAZAGCA&sclient=gws-wiz-serp)) it means that you are almost certainly going to overfit your data. We can see a practical example of this when we learn line of best fit shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonality\n",
    "\n",
    "Vectors are **orthogonal** if their dot product is zero (equivalently, if the angle between them is 90 degrees).\n",
    "\n",
    "**Examples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the basis vectors are orthogonal\n",
    "np.dot(ihat, jhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "northwest = np.array([-1,1])\n",
    "northeast = np.array([1, 1])\n",
    "np.dot(northwest, northeast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If time (if not, next time): Projections\n",
    "\n",
    "A **projection** is a matrix transformation that we can apply to a vector as many times as we'd like and always get the same result. I.e. for the same $b$:\n",
    "\n",
    "- $Ax = b$\n",
    "- $AAx = b$\n",
    "- $AAAx = b$\n",
    "- $\\cdots$\n",
    "\n",
    "**Counterexample and Example**:\n",
    "\n",
    "The matrix $A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}$ (from before) is **NOT** a projection matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = [[2 0]\n",
      " [0 3]]\n",
      "x = [2 4]\n",
      "Ax = [ 4 12]\n",
      "AAx = [ 8 36]\n",
      "AAAx = [ 16 108]\n"
     ]
    }
   ],
   "source": [
    "print(f'A = {A}')\n",
    "print(f'x = {x}')\n",
    "print(f'Ax = {np.matmul(A, x)}')\n",
    "print(f'AAx = {np.matmul(A, np.matmul(A, x))}')\n",
    "print(f'AAAx = {np.matmul(A, np.matmul(A, np.matmul(A, x)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$ **IS** a projection matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = [[1 0]\n",
      " [0 0]]\n",
      "x = [2 4]\n",
      "Ax = [2 0]\n",
      "AAx = [2 0]\n",
      "AAAx = [2 0]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 0],\n",
    "              [0, 0]])\n",
    "print(f'A = {A}')\n",
    "print(f'x = {x}')\n",
    "print(f'Ax = {np.matmul(A, x)}')\n",
    "print(f'AAx = {np.matmul(A, np.matmul(A, x))}')\n",
    "print(f'AAAx = {np.matmul(A, np.matmul(A, np.matmul(A, x)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projection matrices are used to **project** vectors from higher-dimensional space into lower-dimensional space. In the above, the result $\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$ is the **projection of $x$ onto the span of $A$**. Let's back up and look at this slowly:\n",
    "\n",
    "Say you have a vector space defined by the following vectors:\n",
    "\n",
    "$$v = \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix}$$\n",
    "$$w = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$$\n",
    "\n",
    "We previously showed that the span of these vectors is a line, $y = 2x$. Since we know the vectors are multiples of each other, we can actually see that we only need one vector to properly define the span.\n",
    "\n",
    "Let's choose to rename $w$ to be $a = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$. Suppose there is another 2-dimensional vector which we wish to **project onto the span of $a$?** In other words, find the point in the span of $a$ that is closest to this new vector, say $b = \\begin{bmatrix} -2 \\\\ 3 \\end{bmatrix}$.\n",
    "\n",
    "(Visualize this on the board)\n",
    "\n",
    "Let's call:\n",
    "\n",
    "- $p$: the projection of $b$ onto the span of $a$, it is a scaled version of $a$\n",
    "- $e$: the vector orthogonal to $a$ (at 90 degrees) which \"points\" to $b$\n",
    "\n",
    "From this we can write down some facts using vector algebra/definitions:\n",
    "\n",
    "- $p = c \\times a$, where $c$ is some scalar\n",
    "- $b = p + e$, and thus $e = b - p$\n",
    "- $a \\cdot e = 0$, since $a$ and $e$ are orthogonal\n",
    "  - The dot product is also the multiplication of the transpose of $a$ and $e$: $a^Te$ (you can confirm this by hand)\n",
    "\n",
    "From these three facts, we can rearrange things:\n",
    "\n",
    "- $e = b - c \\times a$\n",
    "- $a^T(b - c\\times a) = 0$\n",
    "- $a^Tb - a^Tca = 0$\n",
    "- $c = \\frac{a^Tb}{a^Ta} = \\frac{a \\cdot b}{a \\cdot a}$\n",
    "\n",
    "Which is, in fact, all we need to find the projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.8, 1.6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2, 4])\n",
    "b = np.array([-2, 3])\n",
    "c = np.dot(a, b)/np.dot(a, a)\n",
    "p = c*a\n",
    "print(c)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In higher dimensions\n",
    "\n",
    "Now suppose we want to project $b = \\begin{bmatrix} 3 \\\\ 1 \\\\ 5 \\end{bmatrix}$ onto the span of $a_0 = \\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\end{bmatrix}$ and $a_1 = \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\end{bmatrix}$. We can show that the span is the $x-y$ plane:\n",
    "\n",
    "$$\\alpha \\begin{bmatrix} 0 \\\\ 3 \\\\ 0 \\end{bmatrix} + \\beta \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2\\beta \\\\ 3\\alpha + \\beta \\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "Which implies $z = 0$ (or, no third dimensional values in the span), $y = 3\\alpha + \\frac{x}{2}$ which can be anything for a given $\\alpha$.\n",
    "\n",
    "It should be obvious, because $b$ has a $z$-coordinate of 5, that $b$ is **NOT** in the span of $a_0$ and $a_1$ (it is not on the $x-y$ plane). Let's project it down to that lower-dimensional space.\n",
    "\n",
    "(Can try to draw a picture)\n",
    "\n",
    "Note some similarities to the two dimensional case:\n",
    "\n",
    "- $p = c_0 a_0 + c_1 a_1 = Ac$\n",
    "  - **Note**: here, the two scalars that scale the $a$ vectors are treated as two values in a vector $c$, and the two $a$ vectors are now the columns in a $3\\times2$ matrix, $A$.\n",
    "- $b = p + e$, and thus $e = b - p$\n",
    "- $a_0 \\cdot e = a_1 \\cdot e = 0$\n",
    "  - Or, equivalently, $A^Te = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "Using the same framework as in 2-dimensions, we can again rearrange things:\n",
    "\n",
    "- $e = b - Ac$\n",
    "- $A^T(b - Ac) = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n",
    "- $A^Tb = A^TAc$\n",
    "\n",
    "**But wait!** the goal is to solve for the $c$ vector. In 2-dimensions there was no problem because $a^Tb$ and $a^Ta$ were both scalars; you could divide them. But now, they are matrices, and you **can't divide matrices**. In order to figure out how to solve this, we need:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Inverses (for square matrices)\n",
    "\n",
    "We would like something to \"cancel out\" whatever is in front of $c$, so we can get it by itself. I.e. we want $?$ so that:\n",
    "\n",
    "$$?(A^TA)c = c$$\n",
    "\n",
    "Simplify the problem (the below is the same thing, just with different notation):\n",
    "\n",
    "$$?x = x$$\n",
    "\n",
    "- In general, we'd like to find some matrix $?$ that when you multiply it by $x$ results in only $x$. **Example:** if we want\n",
    "\n",
    "$$\\begin{bmatrix} ? & ? \\\\ ? & ? \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "What must $\\begin{bmatrix} ? & ? \\\\ ? & ? \\end{bmatrix}$ be?\n",
    "\n",
    "$$\\begin{bmatrix} ? & ? \\\\ ? & ? \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I$$\n",
    "\n",
    "This is called the **identity matrix**. We've actually already seen this in numpy (a long time ago...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gives you the 2-dimensional identity matrix\n",
    "I = np.eye(2)\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([3,1])\n",
    "np.matmul(I, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we want $?A^TA = I$...\n",
    "\n",
    "The **inverse** of a square matrix is the matrix that we multiply it by to result in the identity matrix. In other words, for a square matrix $A$:\n",
    "\n",
    "$A^{-1}A = I$\n",
    "\n",
    "And, thus, for the square matrix $A^TA$:\n",
    "\n",
    "$(A^TA)^{-1}A^TA = I$\n",
    "\n",
    "We can find this easily with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2. ,  1. ],\n",
       "       [ 1.5, -0.5]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "Ainv = np.linalg.inv(A)\n",
    "Ainv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(Ainv, A).round()\n",
    "# note I'm rounding above because python does some weird rounding of it's own under the hood...\n",
    "# np.matmul(Ainv, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally**, this means that we can solve for the projection of the point $b = \\begin{bmatrix} 3 \\\\ 1 \\\\ 5 \\end{bmatrix}$ from three-dimensional space down to the span defined by $A = \\begin{bmatrix} 0 & 2 \\\\ 3 & 1 \\\\ 0 & 0 \\end{bmatrix}$, which is the $x-y$ plane:\n",
    "\n",
    "- $A^Tb = A^TAc$\n",
    "- $(A^TA)^{-1}A^Tb = c$\n",
    "- $p = Ac = A(A^TA)^{-1}A^Tb$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 1., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[0, 2],\n",
    "              [3, 1],\n",
    "              [0, 0]])\n",
    "b = np.array([3, 1, 5])\n",
    "AtAinv = np.linalg.inv(np.matmul(A.T, A))\n",
    "c = np.matmul(AtAinv, np.matmul(A.T, b))\n",
    "p = np.matmul(A, c)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which, you'll note, is directly \"under\" $b$ on the $x-y$ plane (if you draw a quick picture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
