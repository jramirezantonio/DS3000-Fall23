{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DS3000 Day 3\n",
    "\n",
    "Sep 19, 2023\n",
    "\n",
    "Admin\n",
    "- Qwickly Attendance (PIN on board)\n",
    "- Homework 1 due Tonight, Sep 19 by midnight\n",
    "- Homework 2 will be posted then, due Oct 10 by midnight\n",
    "      - Note: you have three weeks to do this, but **do not** put it off! The sooner you complete everything the better.\n",
    "- Quiz 1 will be posted **next** Tuesday, Sep 25, and must be done by Oct 3 (2 hour time limit)\n",
    "- Lab and Visitor tentatively scheduled for Sep 25 as well\n",
    "\n",
    "Push-Up Tracker\n",
    "- Section 04: 0\n",
    "- Section 05: 2\n",
    "- Section 06: 1\n",
    "\n",
    "Content:\n",
    "- OpenWeather API pipeline\n",
    "- Intro to Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Pipeline: What is it?\n",
    "\n",
    "A data pipeline is a collection of functions* which split all the functionality of our data collection and processing\n",
    "\n",
    "(*can be other structures too, but it may be easier to first understand each as a function)\n",
    "\n",
    "\n",
    "# Why build a data pipeline?\n",
    "\n",
    "- Allows pipeline to be run in parts (rather than the whole thing)\n",
    "- Allows pipeline to be built by different programmers working on different parts in parallel\n",
    "- Allows us to test each piece of our code seperately\n",
    "- Allows for modification / re-use of different sections\n",
    "\n",
    "What we call a \"Data Pipeline\" here is a specific instance of \"Factoring\" a piece of software, splitting up its functionality into pieces.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OpenWeather API Pipeline Activity\n",
    "\n",
    "OpenWeather API offers a few different queries (see [here](https://openweathermap.org/api) for details):\n",
    "- One Call API (which we have access to)\n",
    "- Solar Radiation API\n",
    "- etc.\n",
    "\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "Build a library of functions which can be pieced together to support the collection, cleaning and display of features from OpenWeather into a scatter plot of two features.\n",
    "\n",
    "### Lets design one together: \n",
    "\n",
    "(think: input/outputs -> handwritten notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plan out a pipeline\n",
    "\n",
    "Write a few 'empty' functions including little more than the docstring:\n",
    "\n",
    "```python\n",
    "def some_fnc(a_string, a_list):\n",
    "    \"\"\" processes a string and a list (somehow)\n",
    "    \n",
    "    Args:\n",
    "        a_string (str): an input string which ...\n",
    "        a_list (list): a list which describes ...\n",
    "        \n",
    "    Returns:\n",
    "        output (dict): the output dict which is ...\n",
    "    \"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "and a script which uses them:\n",
    "\n",
    "```python\n",
    "# inputs (not necessarily complete)\n",
    "lat = -42\n",
    "lon = 73\n",
    "\n",
    "some_output = some_fnc(lat, lon)\n",
    "some_other_output = some_other_fnc(some_output)\n",
    "\n",
    "```\n",
    "\n",
    "which would, if the functions worked, produce a graph like this (note: this starts Oct 6, because I made it last year):\n",
    "\n",
    "<img src=\"https://i.ibb.co/Ct0JtRJ/newplot-1.png\" width=500\\img>\n",
    "\n",
    "**NOTE:** we haven't talked about creating plots yet, but we will next week! For now, I will provide everything you need in the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What might these empty functions look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openweather_onecall(latlon_tuple, api_key, units='imperial'):\n",
    "    \"\"\" returns weather data from one location via onecall\n",
    "    \n",
    "    https://openweathermap.org/api/one-call-api \n",
    "    \n",
    "    Args:\n",
    "        latlon_tuple (tuple): first element is lattitude,\n",
    "            second is longitude            \n",
    "        api_key (str): API key required to access data\n",
    "        units (str): 'imperial', 'standard', 'metric'\n",
    "        \n",
    "    Returns:\n",
    "        weather_dict (dict): a nested dictionary (tree) which\n",
    "            contains weather data\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "def get_clean_df_daily(daily_dict_list):\n",
    "    \"\"\" formats daily_dict to a pandas dataframe\n",
    "    \n",
    "    see https://openweathermap.org/api/one-call-api for\n",
    "    full daily_dict specification\n",
    "    \n",
    "    Args:\n",
    "        daily_dict_list (list): list of dictionaries of daily\n",
    "            weather features\n",
    "            \n",
    "    Returns:\n",
    "        df_daily (pd.DataFrame): each row is weather from one\n",
    "            day\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "def scatter_plotly(df, feat_x, feat_y, f_html='scatter.html'):\n",
    "    \"\"\" creates a plotly scatter plot, exports as html \n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): pandas dataframe\n",
    "        x_feat (str): x axis of scatter\n",
    "        y_feat (str): y axis of scatter\n",
    "        f_html (str): output html file\n",
    "        \n",
    "    Returns:\n",
    "        f_html (str): output html file\n",
    "    \"\"\" \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the pipeline above is complete, the following script should plot a daily max temp scatter for Boston:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code won't work because the functions above are all empty\n",
    "# inputs\n",
    "feat_x = 'date'\n",
    "feat_y = 'temp_max'\n",
    "latlon_tuple = -42, 70\n",
    "units = 'imperial'\n",
    "api_key = 'd36fa352ac73226b30772f64675f41bb'\n",
    "\n",
    "# get data\n",
    "weather_dict = openweather_onecall(latlon_tuple, \n",
    "                                   units=units,\n",
    "                                   api_key=api_key)\n",
    "\n",
    "# clean weather dict (make dataframe from dict, process timestamps etc)\n",
    "df_daily = get_clean_df_daily(weather_dict['daily'])\n",
    "\n",
    "# make scatter\n",
    "f_html = scatter_plotly(df_daily, feat_x=feat_x, feat_y=feat_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's go **SLOWLY** through this solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "\n",
    "def openweather_onecall(latlon_tuple, api_key, units='imperial'):\n",
    "    \"\"\" returns weather data from one location via onecall\n",
    "    \n",
    "    https://openweathermap.org/api/one-call-api \n",
    "    \n",
    "    Args:\n",
    "        latlon_tuple (tuple): first element is lattitude,\n",
    "            second is longitude\n",
    "        api_key (str): API key required to access data\n",
    "        units (str): 'imperial', 'standard', 'metric'\n",
    "        \n",
    "    Returns:\n",
    "        weather_dict (dict): a nested dictionary (tree) which\n",
    "            contains weather data\n",
    "    \"\"\"\n",
    "    # build url\n",
    "    lat, lon = latlon_tuple\n",
    "    url = f'https://api.openweathermap.org/data/2.5/onecall?lat={lat}&lon={lon}&appid={api_key}&units={units}'\n",
    "    \n",
    "    # get url as a string\n",
    "    url_text = requests.get(url).text\n",
    "    \n",
    "    # convert json to a nested dict\n",
    "    weather_dict = json.loads(url_text)\n",
    "\n",
    "    # another, perhaps cleaner option\n",
    "    # weather_dict = requests.get(url).json()\n",
    "    \n",
    "    return weather_dict\n",
    "\n",
    "def get_clean_df_daily(daily_dict_list):\n",
    "    \"\"\" formats daily_dict to a pandas series\n",
    "    \n",
    "    see https://openweathermap.org/api/one-call-api for\n",
    "    full daily_dict specification\n",
    "    \n",
    "    Args:\n",
    "        daily_dict_list (list): list of dictionaries of daily\n",
    "            weather features\n",
    "            \n",
    "    Returns:\n",
    "        df_daily (pd.DataFrame): each row is weather from one\n",
    "            day\n",
    "    \"\"\"\n",
    "    # format to dataframe\n",
    "    df_weather = pd.DataFrame()\n",
    "    for daily_dict in daily_dict_list:\n",
    "        daily_series = pd.Series(dtype='object')\n",
    "\n",
    "        # build datetime data (.fromtimestamp() assumes local time zone)\n",
    "        # todo: timezone problem (left as HW exercise)\n",
    "        daily_series['date'] = datetime.fromtimestamp(daily_dict['dt'])\n",
    "        daily_series['sunrise'] = datetime.fromtimestamp(daily_dict['sunrise'])\n",
    "        daily_series['sunset'] = datetime.fromtimestamp(daily_dict['sunset'])\n",
    "\n",
    "\n",
    "        # build temp data\n",
    "        temp_dict = daily_dict['temp']\n",
    "        for temp_feat, temp in temp_dict.items():\n",
    "            daily_series[f'temp_{temp_feat}'] = temp\n",
    "\n",
    "        # build prob of precipitation\n",
    "        # NOTE: I did confirm that the rain column appears only if there is rain forecasted in the next 48 hours\n",
    "        daily_series['pop'] = daily_dict['pop']\n",
    "                \n",
    "        # collect row in df_weather\n",
    "        df_weather = pd.concat([df_weather, daily_series.to_frame().T])\n",
    "    \n",
    "    return df_weather     \n",
    "\n",
    "def scatter_plotly(df, feat_x, feat_y, f_html='scatter.html'):\n",
    "    \"\"\" creates a plotly scatter plot, exports as html \n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): pandas dataframe\n",
    "        x_feat (str): x axis of scatter\n",
    "        y_feat (str): y axis of scatter\n",
    "        f_html (str): output html file\n",
    "        \n",
    "    Returns:\n",
    "        f_html (str): output html file\n",
    "    \"\"\"\n",
    "    # creat scatter plot\n",
    "    fig = px.scatter(df, x=feat_x, y=feat_y)\n",
    "\n",
    "    \n",
    "    # export scatter to html\n",
    "    plotly.offline.plot(fig, filename=f_html)\n",
    "    \n",
    "    return f_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "feat_x = 'date'\n",
    "feat_y = 'temp_max'\n",
    "latlon_tuple = -42, 70\n",
    "units = 'imperial'\n",
    "api_key = 'd36fa352ac73226b30772f64675f41bb'\n",
    "\n",
    "# get data\n",
    "weather_dict = openweather_onecall(latlon_tuple, \n",
    "                                   units=units,\n",
    "                                   api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean weather dict (make dataframe from dict, process timestamps etc)\n",
    "df_daily = get_clean_df_daily(weather_dict['daily'])\n",
    "df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make scatter\n",
    "f_html = scatter_plotly(df_daily, feat_x=feat_x, feat_y=feat_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "* Using programs or scripts to pretend to browse websites, examine the content on those websites, retrieve and extract data from those websites\n",
    "* Why scrape?\n",
    "    * if an API is available for a service, we will nearly always prefer the API to scraping\n",
    "    * ... but not all services have APIs or the available APIs are too expensive for our project\n",
    "    * newly published information might not yet be available through ready datasets\n",
    "* Downsides of scraping:\n",
    "    * no reference documentation (unlike APIs)\n",
    "    * no guarantee that a webpage we scrape will look and work the same way the next day (might need to rewrite the whole scraper)\n",
    "    * if it violates the terms of service it might be seen as a felony (https://www.aclu.org/cases/sandvig-v-barr-challenge-cfaa-prohibition-uncovering-racial-discrimination-online)\n",
    "    * legal and moral greyzone (even if the ToS does not disallow it, somebody has to pay for the traffic and when you're scraping you're not looking at ads)\n",
    "    * ... but everbody does it anyway (https://www.hollywoodreporter.com/thr-esq/genius-says-it-caught-google-lyricfind-redhanded-stealing-lyrics-400m-suit-1259383)\n",
    "* Web scraping pipeline:\n",
    "    * because the webpages might change their structure it's extra important to keep the crawling/extraction step separate from transformations and loading\n",
    "    * ETL (Extraction-Transform-Load):\n",
    "        * **Crawl**: open a given URL using requests and get the HTML source;\n",
    "        * **Extract**: extract interesting content from the webpage's source.\n",
    "        * **Transform**: our usual unit conversions, etc\n",
    "        * **Load**: representing the data in an easy way for storage and analysis\n",
    "    * **Pro tip**: it's usually a good idea to not only store the transformed data, but also the raw HTML source - because the webpages might change and we might be late to realize we're not extracting right. If we have the original HTML source we can go back to it\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best case scenario\n",
    "Some webpages publish their data in the form of simple tables. In these (rare) cases we can just use pandas .read_html to scrape this data:\n",
    "\n",
    "https://www.espn.com/nba/team/stats/_/name/bos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read html extracts all the <table> elements from html and returns a list of DataFrames created from them\n",
    "tables = pd.read_html('https://www.espn.com/nba/team/stats/_/name/bos')\n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0]\n",
    "# tables[1]\n",
    "# tables[2]\n",
    "# tables[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"glue\" dataframes together (more to come on this later in the semester)\n",
    "player_stats1 = pd.concat(tables[:2], axis=1)\n",
    "player_stats1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the more advanced stats\n",
    "player_stats2 = pd.concat([player_stats1, tables[3]], axis=1)\n",
    "player_stats2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseball instead of basketball?\n",
    "base_tables = pd.read_html('https://www.baseball-reference.com/teams/BOS/2022.shtml')\n",
    "len(base_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tables[0]\n",
    "# base_tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messy Data\n",
    "\n",
    "Notice that the baseball data are quite a bit messier than the basketball data. In web scraping, you are beholden to the format of the website (.html) and will almost certainly have to clean data (sometimes extensively) after scraping it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic HTML\n",
    "Web pages are written in HTML. The source of https://sapiezynski.com/ds3000/scraping/01.html looks like this:\n",
    "\n",
    "```html\n",
    "<html>\n",
    "    <head>\n",
    "        <!-- comments in HTML are marked like this -->\n",
    "        \n",
    "        <!-- the head tag contains the meta information not displayed but helps browsers render the page -->\n",
    "    </head>\n",
    "    <body>\n",
    "         <!-- This is the body of the document that contains all the visible elements.-->\n",
    "        <h1>Heading 1</h1>\n",
    "        <h2>This is what heading 2 looks like</h2>\n",
    "        \n",
    "        <p>Text is usually in paragraphs.\n",
    "            New lines and multiple consecutive whitespace characters are ignored.</p>\n",
    "\n",
    "<p>Unlike in python indentation is only a good practice but it doesn't change functionality. In fact, all of this HTML could be (and often is in real webpages) just writen as a single line.</p>   \n",
    "        \n",
    "        <p>Links are created using the \"a\" tag: \n",
    "            <a href=\"https://www.google.com\">Click here to google.</a>\n",
    "            href is an attirbute of the a tag that specify where the link points to.</p>\n",
    "        \n",
    "        \n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "The keywords in `<>` brackets are called tags. They open with `<tag>` and close with `</tag>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the html content in Python\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://sapiezynski.com/ds3000/scraping/01.html')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes this doesn't quite work the way you want (c'est la vie with web scraping)\n",
    "response2 = requests.get('https://www.nytimes.com/2019/03/10/style/what-is-tik-tok.html')\n",
    "print(response2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup\n",
    "\n",
    "Even if the .html does look relatively clean, it's still just a big string. How can we deal with it? Luckily there is a module made for just this purpose, and it's even a magic command which we can install directly in jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://sapiezynski.com/ds3000/scraping/01.html' \n",
    "str_html = requests.get(url).text\n",
    "soup = BeautifulSoup(str_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting elements by their tag name:\n",
    "soup.find_all('p')\n",
    "\n",
    "# find_all returns a list, where each element is an instance of the specified tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bs4 object tracks the tags\n",
    "type(soup.find_all('p')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in soup.find_all('p'):\n",
    "    # text is a property of a soup object\n",
    "    print(paragraph.text) \n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `.find_all()` on subtrees of soup object\n",
    "\n",
    "\n",
    "The `.find_all()` method works not only on the whole `soup` object, but also on subtrees of the soup object.  \n",
    "\n",
    "Consider the site at https://sapiezynski.com/ds3000/scraping/02.html:\n",
    "\n",
    "```html\n",
    "<html>\n",
    "    <body>\n",
    "        <p>The links in this paragraph point to search engines, like <a href=\"https://duckduckgo.com\">DuckDuckGo</a>, <a href=\"https://google.com\">Google</a>, <a href=\"https://bing.com\">Bing</a></p>\n",
    "        \n",
    "        <p>The links in this paragraph point to Internet browsers, like <a href=\"https://firefox.com\">Firefox</a>, <a href=\"https://chrome.com\">Chrome</a>, <a href=\"https://opera.com\">Opera</a></p>.\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "**Goal**: Grab links from the first paragraph only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the content of the page\n",
    "url = 'https://sapiezynski.com/ds3000/scraping/02.html'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text)\n",
    "\n",
    "# finding all paragraphs:\n",
    "p_all = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the first paragraph\n",
    "p_first = p_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the links from the first paragraph:\n",
    "links_p_first = p_first.find_all('a')\n",
    "\n",
    "print(links_p_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some syntactic sugar: \n",
    "To get the first tag under a soup object, refer to it as an attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is equivilent to soup.find_all('p')[0]\n",
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can condense our code as\n",
    "plinks = soup.p.find_all('a')\n",
    "print(plinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over tags\n",
    "for par in soup.find_all('p'):\n",
    "    print(par.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the first link in that paragraph can be accessed like this:\n",
    "link = soup.p.a\n",
    "print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying if tags exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we're trying to access an element that doesn't exist?\n",
    "header = soup.h3\n",
    "print(header)\n",
    "\n",
    "# won't work, because header is of type None\n",
    "# header.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test if a tag exists in a soup object by looking for the first instance of this tag and comparing it to `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if soup.h3 is None:\n",
    "    print(\"tag h3 doesnt exist in soup\")\n",
    "else:\n",
    "    print(\"tag h3 does exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if soup.p is None:\n",
    "    print(\"tag p doesnt exist in soup\")\n",
    "else:\n",
    "    print(\"tag p does exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding tags by `class_`\n",
    "\n",
    "**Tip**: This is often one of the most useful ways to localize a particular part of a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get soup\n",
    "url = 'https://www.allrecipes.com/search?q=cheese+fondue'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **goal** is to get a list of recipes.  Maybe we should find all the `div` tags? What about `span` tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding via tag ... problematic as we have too many div tags!\n",
    "len(soup.find_all('div'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('span'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags can have multiple \"classes\" they belong to.  For example, in https://www.allrecipes.com/search?q=cheese+fondue the first recipe is encapsulated in this html tag:\n",
    "\n",
    "    <span class=\"card__title\"><span class=\"card__title-text\">Cheese Fondue</span></span>\n",
    "    \n",
    "So this particular span tag belongs to classes:\n",
    "- `card__title`\n",
    "- `card__title-text`\n",
    "    \n",
    "I suspect only our target recipes belong to the `card__title-text` class.  Lets find them all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_list = soup.find_all(class_='card__title-text')\n",
    "\n",
    "len(recipe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_list[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding tags by `id`\n",
    "\n",
    "Nearly the same as finding by class, but you'll look for `id=` in the html and pass it to the `id` keyword of `soup.find_all()`.\n",
    "\n",
    "**Goal**: Get the footer from: https://www.scrapethissite.com/\n",
    "\n",
    "\n",
    "\n",
    "```html\n",
    "<section id=\"footer\">\n",
    "        <div class=\"container\">\n",
    "            <div class=\"row\">\n",
    "                <div class=\"col-md-12 text-center text-muted\">\n",
    "                    Lessons and Videos Â© Hartley Brody 2018\n",
    "                </div><!--.col-->\n",
    "            </div><!--.row-->\n",
    "        </div><!--.container-->\n",
    "    </section>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get soup from url\n",
    "url = 'https://www.scrapethissite.com/'\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(id='footer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can combine all searches shown above:\n",
    "- tag\n",
    "    - p (paragraph)\n",
    "    - a (link)\n",
    "    - div, span, ...\n",
    "- tag class\n",
    "- tag id\n",
    "\n",
    "```python\n",
    "# finds all links (tag type = 'a'), with given class and id\n",
    "soup.find_all('a', class_='fancy-link', id='blue')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice: Rest of Class (if time, if not next time!)\n",
    "\n",
    "**Goal:** Get a list of recipe names from www.allrecipes.com like we did for:\n",
    "\n",
    "https://www.allrecipes.com/search?q=cheese+fondue\n",
    "\n",
    "1. Write function `crawl_recipes(query)` which:\n",
    "    * takes the search phrase (the ingredient) as input argument\n",
    "    * builds the correct url that leads directly to the page that lists the recipes\n",
    "    * uses `requests` to get the content of this page returns the html text of the page\n",
    "1. Write `extract_recipes(text)` which:\n",
    "    * takes the text returned by `crawl_recipes` as argument\n",
    "    * builds a BeautifulSoup object out of that text \n",
    "    * finds names of all recipes\n",
    "        - to identify which tags / classes to `find_all()`, open the page in your browser and \"inspect\" \n",
    "        - start from the recipe object above, and call another `find_all()` to zoom into the recipe name itself\n",
    "    * returns the list of recipe names\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new function that will help if you wish to query multiple words:\n",
    "\n",
    "`string.replace()`\n",
    "\n",
    "So, if you wish to turn `cheese fondue` into `cheese+fondue`:\n",
    "\n",
    "`string = 'cheese fondue'`\n",
    "\n",
    "`string.replace(\" \", \"+\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'cheese fondue'\n",
    "string = string.replace(\" \", \"+\")\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meatloaf_html = crawl_recipes('meatloaf')\n",
    "new_recipe_list = extract_recipes(meatloaf_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_recipe_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
